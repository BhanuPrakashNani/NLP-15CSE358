{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17053_NLP2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWSMOlmCt+9FhjwBpca3+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhanuPrakashNani/NLP-15CSE358/blob/master/17053_NLP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIZnhJL_Lihx",
        "colab_type": "text"
      },
      "source": [
        "NLP ASSIGNMENT 2\n",
        "\n",
        "BHANU PRAKASH POLUPARTHI\n",
        "\n",
        "AM.EN.U4CSE17053"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaSVjDfG8K_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "4e1cc8dc-f3e8-4399-a239-447c70a380eb"
      },
      "source": [
        " #importing nltk and other required modules\n",
        " import nltk, re, pprint\n",
        " nltk.download('popular')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrJkqNLy8bYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "file_1 = open('chatbot.txt',mode='r')\n",
        "tokens = []\n",
        "for line in file_1:\n",
        "  tokens.append(word_tokenize(line))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtpE0TnR-BYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1104e4f9-3397-4034-93ad-8e180358aa37"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Mitsuku',\n",
              "  'claims',\n",
              "  'to',\n",
              "  'be',\n",
              "  'an',\n",
              "  '18-year-old',\n",
              "  'female',\n",
              "  'chatbot',\n",
              "  'from',\n",
              "  'Leeds',\n",
              "  ',',\n",
              "  'England',\n",
              "  '.',\n",
              "  'It',\n",
              "  'contains',\n",
              "  'all',\n",
              "  'of',\n",
              "  'Alice',\n",
              "  \"'s\",\n",
              "  'AIML',\n",
              "  'files',\n",
              "  ',',\n",
              "  'with',\n",
              "  'many',\n",
              "  'additions',\n",
              "  'from',\n",
              "  'user',\n",
              "  'generated',\n",
              "  'conversations',\n",
              "  ',',\n",
              "  'and',\n",
              "  'is',\n",
              "  'always',\n",
              "  'a',\n",
              "  'work',\n",
              "  'in',\n",
              "  'progress',\n",
              "  '.',\n",
              "  'Worswick',\n",
              "  'claims',\n",
              "  'she',\n",
              "  'has',\n",
              "  'been',\n",
              "  'worked',\n",
              "  'on',\n",
              "  'since',\n",
              "  '2005',\n",
              "  '.',\n",
              "  '[',\n",
              "  '4',\n",
              "  ']'],\n",
              " [],\n",
              " ['Her',\n",
              "  'intelligence',\n",
              "  'includes',\n",
              "  'the',\n",
              "  'ability',\n",
              "  'to',\n",
              "  'reason',\n",
              "  'with',\n",
              "  'specific',\n",
              "  'objects',\n",
              "  '.',\n",
              "  'For',\n",
              "  'example',\n",
              "  ',',\n",
              "  'if',\n",
              "  'someone',\n",
              "  'asks',\n",
              "  '``',\n",
              "  'Can',\n",
              "  'you',\n",
              "  'eat',\n",
              "  'a',\n",
              "  'house',\n",
              "  '?',\n",
              "  '``',\n",
              "  ',',\n",
              "  'Mitsuku',\n",
              "  'looks',\n",
              "  'up',\n",
              "  'the',\n",
              "  'properties',\n",
              "  'for',\n",
              "  '``',\n",
              "  'house',\n",
              "  \"''\",\n",
              "  '.',\n",
              "  'Finds',\n",
              "  'the',\n",
              "  'value',\n",
              "  'of',\n",
              "  '``',\n",
              "  'made_from',\n",
              "  \"''\",\n",
              "  'is',\n",
              "  'set',\n",
              "  'to',\n",
              "  '``',\n",
              "  'brick',\n",
              "  \"''\",\n",
              "  'and',\n",
              "  'replies',\n",
              "  '``',\n",
              "  'no',\n",
              "  \"''\",\n",
              "  ',',\n",
              "  'as',\n",
              "  'a',\n",
              "  'house',\n",
              "  'is',\n",
              "  'not',\n",
              "  'edible',\n",
              "  '.'],\n",
              " [],\n",
              " ['She',\n",
              "  'can',\n",
              "  'play',\n",
              "  'games',\n",
              "  'and',\n",
              "  'do',\n",
              "  'magic',\n",
              "  'tricks',\n",
              "  'at',\n",
              "  'the',\n",
              "  'user',\n",
              "  \"'s\",\n",
              "  'request',\n",
              "  '.',\n",
              "  'In',\n",
              "  '2015',\n",
              "  'she',\n",
              "  'conversed',\n",
              "  ',',\n",
              "  'on',\n",
              "  'average',\n",
              "  ',',\n",
              "  'in',\n",
              "  'excess',\n",
              "  'of',\n",
              "  'a',\n",
              "  'quarter',\n",
              "  'of',\n",
              "  'a',\n",
              "  'million',\n",
              "  'times',\n",
              "  'daily',\n",
              "  '.',\n",
              "  '[',\n",
              "  '5',\n",
              "  ']'],\n",
              " [],\n",
              " ['In',\n",
              "  'a',\n",
              "  'Wall',\n",
              "  'Street',\n",
              "  'Journal',\n",
              "  'article',\n",
              "  'titled',\n",
              "  '“',\n",
              "  'Advertising',\n",
              "  '’',\n",
              "  's',\n",
              "  'New',\n",
              "  'Frontier',\n",
              "  ':',\n",
              "  'Talk',\n",
              "  'to',\n",
              "  'the',\n",
              "  'Bot',\n",
              "  ',',\n",
              "  '”',\n",
              "  'technology',\n",
              "  'reporter',\n",
              "  'Christopher',\n",
              "  'Mims',\n",
              "  'made',\n",
              "  'the',\n",
              "  'case',\n",
              "  'for',\n",
              "  '“',\n",
              "  'chatvertising',\n",
              "  '”',\n",
              "  'in',\n",
              "  'a',\n",
              "  'piece',\n",
              "  'about',\n",
              "  'Mitsuku',\n",
              "  'and',\n",
              "  'Kik',\n",
              "  'Messenger',\n",
              "  ':'],\n",
              " [],\n",
              " ['If',\n",
              "  'it',\n",
              "  'seems',\n",
              "  'improbable',\n",
              "  'that',\n",
              "  'so',\n",
              "  'many',\n",
              "  'teens—80',\n",
              "  '%',\n",
              "  'of',\n",
              "  'Kik',\n",
              "  \"'s\",\n",
              "  'users',\n",
              "  'are',\n",
              "  'under',\n",
              "  '22—would',\n",
              "  'want',\n",
              "  'to',\n",
              "  'talk',\n",
              "  'to',\n",
              "  'a',\n",
              "  'robot',\n",
              "  ',',\n",
              "  'consider',\n",
              "  'what',\n",
              "  'the',\n",
              "  'creator',\n",
              "  'of',\n",
              "  'an',\n",
              "  'award-winning',\n",
              "  ',',\n",
              "  'Web-accessible',\n",
              "  'chat',\n",
              "  'bot',\n",
              "  'named',\n",
              "  'Mitsuku',\n",
              "  'told',\n",
              "  'an',\n",
              "  'interviewer',\n",
              "  'in',\n",
              "  '2013',\n",
              "  '.',\n",
              "  '``',\n",
              "  'What',\n",
              "  'keeps',\n",
              "  'me',\n",
              "  'going',\n",
              "  'is',\n",
              "  'when',\n",
              "  'I',\n",
              "  'get',\n",
              "  'emails',\n",
              "  'or',\n",
              "  'comments',\n",
              "  'in',\n",
              "  'the',\n",
              "  'chat-logs',\n",
              "  'from',\n",
              "  'people',\n",
              "  'telling',\n",
              "  'me',\n",
              "  'how',\n",
              "  'Mitsuku',\n",
              "  'has',\n",
              "  'helped',\n",
              "  'them',\n",
              "  'with',\n",
              "  'a',\n",
              "  'situation',\n",
              "  'whether',\n",
              "  'it',\n",
              "  'was',\n",
              "  'dating',\n",
              "  'advice',\n",
              "  ',',\n",
              "  'being',\n",
              "  'bullied',\n",
              "  'at',\n",
              "  'school',\n",
              "  ',',\n",
              "  'coping',\n",
              "  'with',\n",
              "  'illness',\n",
              "  'or',\n",
              "  'even',\n",
              "  'advice',\n",
              "  'about',\n",
              "  'job',\n",
              "  'interviews',\n",
              "  '.',\n",
              "  'I',\n",
              "  'also',\n",
              "  'get',\n",
              "  'many',\n",
              "  'elderly',\n",
              "  'people',\n",
              "  'who',\n",
              "  'talk',\n",
              "  'to',\n",
              "  'her',\n",
              "  'for',\n",
              "  'companionship',\n",
              "  '.',\n",
              "  \"''\",\n",
              "  'Any',\n",
              "  'advertiser',\n",
              "  'who',\n",
              "  'does',\n",
              "  \"n't\",\n",
              "  'sit',\n",
              "  'bolt',\n",
              "  'upright',\n",
              "  'after',\n",
              "  'reading',\n",
              "  'that',\n",
              "  'does',\n",
              "  \"n't\",\n",
              "  'understand',\n",
              "  'the',\n",
              "  'dark',\n",
              "  'art',\n",
              "  'of',\n",
              "  'manipulation',\n",
              "  'on',\n",
              "  'which',\n",
              "  'their',\n",
              "  'craft',\n",
              "  'depends',\n",
              "  '.',\n",
              "  '[',\n",
              "  '6',\n",
              "  ']'],\n",
              " [],\n",
              " ['Mitsuku',\n",
              "  'has',\n",
              "  'been',\n",
              "  'featured',\n",
              "  'in',\n",
              "  'a',\n",
              "  'number',\n",
              "  'of',\n",
              "  'other',\n",
              "  'news',\n",
              "  'outlets',\n",
              "  '.',\n",
              "  'Fast',\n",
              "  'Company',\n",
              "  'described',\n",
              "  'Mitsuku',\n",
              "  'as',\n",
              "  '“',\n",
              "  'quite',\n",
              "  'impressive',\n",
              "  '”',\n",
              "  'and',\n",
              "  'declared',\n",
              "  'her',\n",
              "  'the',\n",
              "  'victor',\n",
              "  'over',\n",
              "  'Siri',\n",
              "  'in',\n",
              "  'a',\n",
              "  'chatbot',\n",
              "  'smackdown',\n",
              "  '.',\n",
              "  '[',\n",
              "  '7',\n",
              "  ']',\n",
              "  'A',\n",
              "  'blog',\n",
              "  'post',\n",
              "  'for',\n",
              "  'the',\n",
              "  'Guardian',\n",
              "  'on',\n",
              "  'loneliness',\n",
              "  'explored',\n",
              "  'the',\n",
              "  'role',\n",
              "  'chatbots',\n",
              "  'like',\n",
              "  'Mitsuku',\n",
              "  'and',\n",
              "  'Microsoft',\n",
              "  '’',\n",
              "  's',\n",
              "  'XiaoIce',\n",
              "  '[',\n",
              "  '8',\n",
              "  ']',\n",
              "  'play',\n",
              "  'as',\n",
              "  'companions',\n",
              "  ',',\n",
              "  'rather',\n",
              "  'than',\n",
              "  'mere',\n",
              "  'assistants',\n",
              "  ',',\n",
              "  'in',\n",
              "  'peoples',\n",
              "  \"'\",\n",
              "  'emotional',\n",
              "  'lives',\n",
              "  '.',\n",
              "  '[',\n",
              "  '9',\n",
              "  ']']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCE05x8S-IUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "default_stopwords = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh4_9A63JAgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt = [inner for outer in tokens for inner in outer]\n",
        "tt = [word.lower() for word in tt if len(word) > 3]\n",
        "tt = [word for word in tt if word not in default_stopwords]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IFUsqmY-igU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "9fed3902-7657-49c2-c0d3-8850ccfa275e"
      },
      "source": [
        "fdist = nltk.FreqDist(tt)\n",
        "for word, frequency in fdist.most_common(10):\n",
        "    print(u'{};{}'.format(word, frequency))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mitsuku;8\n",
            "many;3\n",
            "house;3\n",
            "talk;3\n",
            "claims;2\n",
            "chatbot;2\n",
            "user;2\n",
            "play;2\n",
            "people;2\n",
            "advice;2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlLDk4ZYIQ91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "a0fa4f87-4aa0-4b94-8c78-c5e04a60046a"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "stems = []\n",
        "for word,frequency in fdist.most_common(10):\n",
        "  print(word, \" : \", ps.stem(word))\n",
        "  stems.append(ps.stem(word))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mitsuku  :  mitsuku\n",
            "many  :  mani\n",
            "house  :  hous\n",
            "talk  :  talk\n",
            "claims  :  claim\n",
            "chatbot  :  chatbot\n",
            "user  :  user\n",
            "play  :  play\n",
            "people  :  peopl\n",
            "advice  :  advic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQ05t-rJBvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "eb1b62b1-63b9-4d72-c3a0-d7f8ad81415b"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lem = []\n",
        "\n",
        "for word,frequency in fdist.most_common(10):\n",
        "  print(word, \" : \", lemmatizer.lemmatize(word))\n",
        "  lem.append(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mitsuku  :  mitsuku\n",
            "many  :  many\n",
            "house  :  house\n",
            "talk  :  talk\n",
            "claims  :  claim\n",
            "chatbot  :  chatbot\n",
            "user  :  user\n",
            "play  :  play\n",
            "people  :  people\n",
            "advice  :  advice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bArLQlXCJwq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "e363ddef-5236-45ae-feba-ce56823e964c"
      },
      "source": [
        "x = fdist.most_common(10)\n",
        "for i in range(len(stems)):\n",
        "  if stems[i] != x[i][0]:\n",
        "    print(\"Not Stemmed word: \", x[i][0], \" || Stemmed Word\", stems[i])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not Stemmed word:  many  || Stemmed Word mani\n",
            "Not Stemmed word:  house  || Stemmed Word hous\n",
            "Not Stemmed word:  claims  || Stemmed Word claim\n",
            "Not Stemmed word:  people  || Stemmed Word peopl\n",
            "Not Stemmed word:  advice  || Stemmed Word advic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGu4cw6K0Cl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bb8c81f-984a-4700-a3c9-b868203aa02c"
      },
      "source": [
        "x = fdist.most_common(10)\n",
        "for i in range(len(lem)):\n",
        "  if lem[i] != x[i][0]:\n",
        "    print(\"Not Lemmatized word: \", x[i][0], \" || Lemmatized Word\", lem[i])\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not Lemmatized word:  claims  || Lemmatized Word claim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08UxWqCjLQWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}